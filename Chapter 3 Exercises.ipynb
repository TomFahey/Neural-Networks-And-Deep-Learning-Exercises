{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Improving the way neural networks learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*Verify that $\\sigma^\\prime (z) = \\sigma (z)(1-\\sigma(z))$.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^\\prime &= \\frac{\\partial \\sigma (z_j^L)}{\\partial z_j^L} \\\\\n",
    "&= \\frac{\\partial}{\\partial z_j^L} \\left( \\frac{1}{1+e^{-z_j^L}} \\right ) \\\\\n",
    "&= \\frac{\\partial}{\\partial z_j^L} \\left( \\frac{e^{z_j^L}}{e^{z_j^L}+1} \\right ) \\\\\n",
    "&= \\frac{(e^{z_j^L} + 1)(e^{z_j^L}) - (e^{z_j^L})^2}{(e^{z_j^L}+1)^2} \\\\\n",
    "&= \\frac{e^{z_j^L}}{e^{z_j^L}+1} - \\left ( \\frac{e^{z_j^L}}{e^{z_j^L} + 1} \\right )^2 \\\\\n",
    "&= \\sigma - \\sigma^2 \\\\\n",
    "&= \\sigma(1-\\sigma)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*One gotcha with the cross-entropy is that it can be difficult at first to remember the respective roles of the $y$s and the $a$s. It's easy to get confused about whether the right form is $-[y\\ln a + (1-y)\\ln (1-a)]$ or $-[a\\ln y + (1-a)\\ln (1-y)]$. What happens to the second of these expressions when $y = 0$ or $1$? Does this problem afflict the first expression? Why or why not?*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The $\\ln(0)$ is not defined, so when $y=\\{0,1\\}$, the second   expression would not be defined. This is not an issue in the first expression, as $y$ does not feature in the $\\ln()$ terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*In the single-neuron discussion at the start of this section, I argued that the cross-entropy is small if $\\sigma(z)\\approx y$. for all training inputs. The argument relied on $y$ being equal to either $0$ or $1$. This is usually true in classification problems, but for other problems (e.g., regression problems) y can sometimes take values intermediate between $0$ and $1$. Show that the cross-entropy is still minimized when $\\sigma (z) = y$ for all training inputs. When this is the case, the cross-entropy has the value:*\"\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C = - \\frac{1}{n} \\sum_x [y\\ln y + (1-y)\\ln (1-y)].\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The definition of the cost function was given by:\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "C &= - \\frac{1}{n} \\sum_x [y\\ln a + (1-y)\\ln (1-a)] \\\\\n",
    "&= - \\frac{1}{n} \\sum_x [y\\ln \\sigma + (1-y)\\ln (1-\\sigma)].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $\\sigma$,\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial \\sigma} &= \\frac{\\partial}{\\partial \\sigma} \\left (- \\frac{1}{n}\\sum_x [y\\ln \\sigma + (1-y)\\ln (1-\\sigma)] \\right ) \\\\\n",
    "&= - \\frac{1}{n} \\sum_x [\\frac{y}{\\sigma} + (1-y)\\left (\\frac{-1}{1-\\sigma} \\right ) ]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The derivative is equal to zero when $C$ is minimized, which we can see will be the case when $\\sigma = y$.\n",
    "\n",
    "The quantity $-[y\\ln y + (1-y)\\ln(1-y)]$ is sometimes know as the binary entropy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Many-layer multi-neuron networks\n",
    "\n",
    "\"*In the notation introduced in the last chapter, show that for the quadratic cost the partial derivative with respect to weights in the output layer is*\"\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} = \\frac{1}{n} \\sum_x a_k^{L-1}(a_j^L-y_j)\\sigma^\\prime(z_j^L)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Starting with the definition of the quadratic cost function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C = \\frac{1}{2n} \\sum_x {|| \\vec{y}(x) - \\vec{a^L}(x) ||}^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Use of the chain rule yields,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} = \\frac{\\partial C}{\\partial z_j^L}\\frac{\\partial z_j^L}{\\partial w_{jk}^L},\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for which the partial derivatives are given by,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial z_j^L} &= \\frac{2}{2n} \\sum_x \\big[\\big(y_j - \\sigma z_j^L\\big) \\big(-\\sigma^\\prime\\big)\\big] \\\\\n",
    "&= \\frac{1}{n} \\sum_x \\big[\\sigma^\\prime(a_j^L - y_j)\\big], \\\\\n",
    "\\frac{\\partial z_j^L}{\\partial w_{jk}^L} &= \\frac{\\partial}{\\partial w_{jk}^L} \\bigg(\\sum_m \\big[w_{jm}^L a_m^{L-1}\\big] + b_j^L \\bigg), \\\\\n",
    "&= \\frac{\\partial}{\\partial w_{jk}^L} \\bigg(w_{jk}a_{k}^{L-1} +b_j^L\\bigg)= a_k^{L-1},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "giving the result,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} = \\frac{1}{n}\\sum_x \\big[a_k^{L-1}(a_j^L - y_j)\\sigma^\\prime\\big]\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*The term $\\sigma^\\prime(z_j^L)$ causes a learning slowdown whenever an output neuron saturates on the wrong value.* \n",
    "\n",
    "*Show that for the cross-entropy cost the output error $\\delta^L$ for a single training example x is given by*\"\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta^L = a^L - y\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The output error $\\delta^L$ was defined as:\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^L &\\equiv \\frac{\\partial C}{\\partial z_j^L},\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "For the cross-entropy cost function, this is then:\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^L &\\equiv \\frac{\\partial C}{\\partial z_j^L} \\\\\n",
    "&= \\frac{\\partial}{\\partial z_j^L} \\bigg ( -y\\ln a -(1-y)\\ln(1-a) \\bigg ) \\\\\n",
    "&= \\frac{\\partial}{\\partial z_j^L} \\bigg (\\big(y-1\\big)\\ln\\big(1-\\sigma(z_j^L)\\big) - y\\ln \\sigma(z_j^L) \\bigg ) \\\\\n",
    "&= (y-1)\\frac{-\\sigma^\\prime}{1-\\sigma} - y\\frac{\\sigma^\\prime}{\\sigma} \\\\\n",
    "&= (1-y)\\frac{\\sigma(1-\\sigma)}{(1-\\sigma)} - y\\frac{\\sigma(1-\\sigma)}{\\sigma}\\\\\n",
    "&= \\sigma(1-y)  - y(1-\\sigma)\\\\\n",
    "&= \\sigma - y \\\\\n",
    "&= a^L - y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*Use this expression to show that the partial derivative with respect to the weights in the output layer is given by,*\"\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} = \\frac{1}{n} \\sum_k a_k^{L-1}(a_j^L-y_j).\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The cross-entropy cost function is defined as:\n",
    "    \n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\frac{-1}{n} \\sum_x [y\\ln a + (1-y)\\ln(1-a)], \\\\\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} &= \\frac{\\partial C}{\\partial a_j^L}\\frac{\\partial a_j^L}{\\partial w_{jk}^L}, \\\\\\\\\n",
    "\\frac{\\partial C}{\\partial a_j^L} &= \\frac{-1}{n} \\sum_x \\left[\\frac{y}{a}+\\frac{(1-y)(-1)}{1-a}\\right], \\\\\\\\\n",
    "\\frac{\\partial a_j^L}{\\partial w_{jk}^L} &= \\frac{\\partial a_j^L}{\\partial z_j^L} \\frac{\\partial z_j^L}{\\partial w_{jk}^L} \\\\\n",
    "&= \\sigma^\\prime(z_j^L)a_k^{L-1} \\\\\n",
    "&= \\sigma(1-\\sigma)a_k^{L-1}, \\\\\\\\\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} &= \\frac{-1}{n} \\sum_x \\left[\\left (\\frac{y}{a}+\\frac{(1-y)(-1)}{1-a}\\right ) \\sigma(1-\\sigma)a_k^{L-1}\\right] \\\\\n",
    "&= \\frac{-1}{n} \\sum_x \\left[\\left (\\frac{y}{a}+\\frac{(1-y)(-1)}{1-a}\\right ) a(1-a)a_k^{L-1}\\right] \\\\\n",
    "&= \\frac{1}{n} \\sum_x \\left[a_k^{L-1}\\Big ( (1-y)\\sigma - y(1-\\sigma) \\Big ) \\right] \\\\\n",
    "&= \\frac{1}{n} \\sum_x a_k^{L-1}(a_j^L - y_j) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "The $\\sigma^\\prime(z_j^L)$ term has vanished, and so the cross-entropy avoids the problem of learning slowdown, not just when used with a single neuron, as we saw earlier, but also in many-layer multi-neuron networks. A simple variation on this analysis holds also for the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*Construct an example showing explicitly that in a network with a sigmoid output layer, the output activations $a_j^L$ won't always sum to 1.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     1,
     8
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of output activations: 2.6874868309338185 > 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "sigmoid(1)\n",
    "z_L = np.random.random(4)\n",
    "a_L = sigmoid(z_L)\n",
    "summation = np.sum(a_L)\n",
    "rel_val = summation > 1\n",
    "if rel_val:\n",
    "    print('Sum of output activations: ' + str(summation) + ' > 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Monoticity of softmax\n",
    "\n",
    "\"*Show that $\\partial a_j^L / \\partial z_k^L$ is positive if $z=k$ and negative if $j\\neq k$. As a consequence, increasing $z_j^L$ is guaranteed to increase the corresponding outputactivation, $a_j^L$ and will decrease the other output activations. We already saw this empirically with the sliders, but this is a rigorous proof.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Starting with the definition of the softmax function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_j^L &= \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "when $j=k$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial a_j^L}{\\partial z_k^L} &= \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\\n",
    "\\tag{A.1}\n",
    "&= \\frac{\\partial}{\\partial z_j^L} \\left ( \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}} \\right ) \\\\\n",
    "\\tag{A.2}\n",
    "&= \\frac{(\\sum_k e^{z_k^L})(e^{z_j^L})-{(e^{z_j^L})}^2}{{(\\sum_k e^{z_k^L})}^2} \\\\\n",
    "&= \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}} - \\frac{{e^{z_j^L}}^2}{(\\sum_k e^{z_k^L})^2} \\\\\n",
    "\\tag{A.3}\n",
    "&= a_j^L - (a_j^L)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "when $j\\neq k$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tag{A.4}\n",
    "\\frac{\\partial a_j^L}{\\partial z_k^L} &= e^{z_j^L}\\frac{\\partial}{\\partial z_k^L} \\left ( \\frac{1}{\\sum_k e^{z_k^L}} \\right ) \\\\\n",
    "\\tag{A.5}\n",
    "&= e^{z_j^L} \\left ( \\frac{(0)-(e^{z_k^L})}{{(\\sum_k e^{z_k^L})}^2} \\right ) \\\\\n",
    "\\tag{A.6}\n",
    "&= -a_k^La_j^L\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since we know that $0 < \\{a_j^L, a_k^L\\} < 1$ for all $\\{j,k\\}$, this implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{when $j=k$, from $\\text{(A.3)}$:  } \\frac{\\partial a_j^L}{\\partial z_k^L} > 0 \\\\\n",
    "\\text{when $j\\neq k$, from $\\text{(A.6)}$:  } \\frac{\\partial a_j^L}{\\partial z_k^L} < 0 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Non-locality of softmax\n",
    "\n",
    "\"*A nice thing about sigmoid layers is that the output $a_j^L$ is a function of the corresponding weighted input, $a_j^L = \\sigma(z_j^L)$. Explain why this is not the case for a softmax layer: any particular output activation $a_j^L$ depends on **all** the weighted inputs.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the sigmoid function, $\\partial a_J^L / \\partial z_k^L$ is equal to $0$. For the softmax function, it is not equal to zero, thus $a_j^L$ depends on all the weighted inputs, $z_k^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Inverting the softmax layer\n",
    "\n",
    "\"*Suppose we have a neural network with a softmax output layer, and the activations $a_j^L$ are known. Show that the corresponding weighted inputs have the form $z_j^L = \\ln a_j^L + \\kappa$, for some constant $\\kappa$ that is independent of $j$.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Starting with the definition of the softmax function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_j^L &= \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "taking the natural logarithm of both sides yields,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln a_j^L &= \\ln \\left (\\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}} \\right ) \\\\\n",
    "& = z_j^L - \\ln \\sum_k e^{z_k^L}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Re-arranging, we have,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z_j^L &= \\ln a_j^L + \\ln \\sum_k e^{z_k^L} \\\\\n",
    "&= \\ln a_j^L + \\kappa\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*Derive Equations () and ()*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Starting with the definition of log-likelihood cost function, we can re-express it in terms of $z_j^L$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C &\\equiv - \\ln a_y^L \\\\\n",
    "&= -\\ln \\left ( \\frac{e^{z_j^L}}{\\sum_k e^{z_k^L}} \\right ) \\\\\n",
    "&= -\\ln e^{z_j^L} + \\ln \\left(\\sum_k e^{z_k^L}\\right) \\\\\n",
    "&= \\ln \\left(\\sum_k e^{z_k^L}\\right) - z_j^L\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking the derivative of C with respect to $z_j^L$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial z_j^L} &=  \\frac{\\partial C}{\\partial z_j^L} \\left ( \\ln \\sum_k {e^{z_k^L}} \\right ) - 1 \\\\\n",
    "&= \\frac{e^{z_J^L}}{\\sum_k e^{z_j^L}} - 1 \\\\\n",
    "&= a_j^L - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Re-writing the partial derivative with respect to $b_j^L$ in terms of $z_j^L$ using the chain rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial b_j^L} &= \\frac{\\partial C}{\\partial z_j^L}\\frac{\\partial z_j^L}{\\partial b_j^L} \\\\\n",
    "&= ( a_j^L - 1 ) \\frac{\\partial z_j^L}{\\partial b_j^L} \\\\\n",
    "&= a_j^L - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Likewise with the derivative with respect to $w_{jk}^L$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w_{jk}^L} &= \\frac{\\partial C}{\\partial z_j^L}\\frac{\\partial z_j^L}{\\partial w_{jk}^L} \\\\\n",
    "&= ( a_j^L - 1 ) \\frac{\\partial z_j^L}{\\partial w_{jk}^L} \\\\\n",
    "&= a_k^{L-1}(a_j^L - 1)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Where does the \"softmax\" name come from? \n",
    "\n",
    "\"*Suppose we change the softmax function so the output activations are given by*\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a_j^L = \\frac{e^{cz_j^L}}{\\sum_k e^{cz_k^L}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "  *where $c$ is a positive constant. Note that $c=1$ corresponds to the standard softmax function. But if we use a different value of $c$ we get a different function, which is nonetheless qualitatively rather similar to the softmax. In particular, show that the output activations form a probability distribution, just as for the usual softmax. Suppose we allow $c$ to become large, i.e. $c \\rightarrow \\infty$. What is the limiting value for the output activations $a_j^L$? After solving this problem it should be clear to you why we think of the $c=1$ function as a \"softened\" version of the maximum function. This is the origin of the term \"softmax\".*\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we re-write $c$ as $1/k_bT$ then the form of the output activations will look very similar to the Boltzmann distribution of occupancy of states at different energy levels, where $T$ is analogous to the temperature and the $z_k^L$ the (negative) energy level of each state. In fact, the mathematics is exactly the same, as shown:\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "a_j^L &= \\frac{e^{cz_j^L}}{\\sum_k e^{cz_k^L}} \\\\\n",
    "&=\\frac{1}{\\sum_k e^{c(z_k^L - z_j^L)}} \\\\\n",
    "&=\\frac{1}{1 + \\sum_{k\\neq j} e^{c(z_k^L - z_j^L)}} \\\\\n",
    "&= \\begin{cases} 1 &\\mbox{if } z_j^L > z_k^L \\text{ for all k} \\\\ 0 &\\mbox{if } z_j^L < z_k^L \\text{ for any k} \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "This is analogous to the Boltzmann probability distribution, $f(\\epsilon_j,T)$, for the occupancy of states with energy $\\epsilon_j$, at temperature $T$ which, in the limit that $T\\rightarrow 0$, collapses to an occupation of 1 in the lowest energy state, and 0 at all other states:\n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\epsilon_j,T) &= \\frac{e^{-\\epsilon_j/k_bT}}{\\sum_k e^{-\\epsilon_k/k_bT}} \\\\\n",
    "&=\\frac{1}{\\sum_k e^{-(\\epsilon_j-\\epsilon_k)/k_bT}} \\\\\n",
    "&=\\frac{1}{1 + \\sum_{k\\neq j} e^{-(\\epsilon_j-\\epsilon_k)/k_bT}} \\\\\n",
    "\\lim_{T \\to 0} f(\\epsilon_j) &= \\begin{cases} 1 &\\mbox{if } \\epsilon_j < \\epsilon_k \\text{ for all k} \\\\ 0 &\\mbox{if } \\epsilon_j > \\epsilon_k \\text{ for any k} \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So the softmax function is essentially just the Boltzmann distribution, where the exponential terms become positive, resulting in the limiting cases being reversed, i.e the probability becomes 1 for the neuron with the largest value of $z^L$ and 0 for all others, which makes sense when you consider that $-\\epsilon_j \\rightarrow z_j^L$ is equivalent to the condition \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\epsilon_j < \\epsilon_k &\\rightarrow -\\epsilon_j < -\\epsilon_k \\\\\n",
    "&= \\epsilon_j > \\epsilon_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "i.e $z_j^L > z_k^L$ due to the signs being reversed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Backpropagation with softmax and the log-likelihood cost\n",
    "\n",
    "\"*In the last chapter we derived the backpropagation algorithm for a network containing sigmoid layers. To apply the algorithm to a network with a softmax layer we need to figure out an expression for the error  $\\delta_j^L \\equiv \\partial C / \\partial z_j^L$ in the final layer. Show that a suitable expression is:*\"\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^L = a_j^L - y_j\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See previous answer\n",
    "\n",
    "*Using this expression we can apply the backpropagation algorithm to a network using a softmax output layer and the log-likelihood cost.*\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) \n",
    "\n",
    "\"*As discussed above, one way of expanding the MNIST training data is to use small rotations of training images. What's a problem that might occur if we allow arbitrarily large rotations of training images?*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Arbitrary rotations would include the possibility of rotationally symmetric values such as 66 and 99 being incorrectly identified as each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Problem\n",
    "\n",
    "**(Research problem)** *How do our machine learning algorithms perform in the limit of very large data sets? For any given algorithm it's natural to attempt to define a notion of asymptotic performance in the limit of truly big data. A quick-and-dirty approach to this problem is to simply try fitting curves to graphs like those shown above, and then to extrapolate the fitted curves out to infinity. An objection to this approach is that different approaches to curve fitting will give different notions of asymptotic performance. Can you find a principled justification for fitting to some particular class of curves? If so, compare the asymptotic performance of several different machine learning algorithms.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "\"*Verify that the standard deviation of $z = \\sum_j w_jx_j + b$ in the paragraph above is $\\sqrt{3/2}$. It may help to know that: (a) the variance of a sum of independent random variables is the sum of the variance of the individual random variables; and (b) the variance is the square of the standard deviation.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "Var(z) &= \\sum_j Var(w_j x_j) + Var(b) \\\\\n",
    "&= \\frac{n_{in}}{2n_{in}} + 1 \\\\\n",
    "&= \\frac{3}{2} \\\\\n",
    "\\sigma_z &= \\sqrt{Var(z)} \\\\\n",
    "&= \\sqrt{3/2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q) Connecting regularization and the improved method of weight initialization\n",
    "\n",
    "\"*L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization. Suppose we are using the old approach to weight initialization. Sketch a heuristic argument that:* \n",
    "\n",
    "1) *supposing $\\lambda$ is not too small, the first epochs of training will be dominated almost entirely by weight decay;*\n",
    "\n",
    "2) *provided $n\\lambda << n$ the weights will decay by a factor of $exp(-\\eta \\lambda / m)$ per epoch; and*\n",
    "\n",
    "3)*supposing $\\lambda$ is not too large, the weight decay will tail off when the weights are down to a size around $1/n^{1/2}$, where $n$ is the total number of weights in the network. Argue that these conditions are all satisfied in the examples graphed in this section.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**(1)** Under the old approach to weight initialization, we saw that we encounter the problem of a broad distribution for the value of $z = \\sum_j w_j x_j + b$, e.g\n",
    "  \n",
    "<img src='capture_Tue_Dec__4 23_19_49_GMT_2018.png'>\n",
    "\n",
    "where the value of $z$ can be either $z \\gt \\gt 1$ or $z \\lt \\lt -1$, which in turn results in the value of the output $\\sigma(z)$ from the hidden neuron will be very close to either 1 or 0 i.e it will have saturated. As we saw from $\\text{BP1}$, the error $\\delta^l$ is proportional to $\\sigma^\\prime$, so for saturated neurons, where the value of $\\sigma$ is very close to $0$ or $1$ and consequently, $\\sigma^\\prime(z)$ is very small, the value of $\\partial C / \\partial w_j^l$ will be very small and so making small changes in the weights will barely change the activation of the hidden neuron. It therefore stands to reason that, in the update equation,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w &\\rightarrow w - \\eta\\frac{\\partial C_0}{\\partial w} - \n",
    "\\frac{\\eta \\lambda}{n}w \\\\\n",
    "&= \\left (1 - \\frac{\\eta \\lambda}{n}\\right)w - \\eta\\frac{\\partial \n",
    "C_0}{\\partial w}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "the term on the right will be very small at first and so the term on the left dominates, resulting in the first few epochs of training mostly consisting of weight decay, until the weights, $w$, have shrunk to the extent that the standard deviation, $\\sigma_z$, has become small enough that the value of $z$ is much more tightly peaked about 0:\n",
    "  \n",
    "<img src='capture_Wed_Dec__5 14_01_03_GMT_2018.png'>\n",
    "  \n",
    "At this point, the sigmoid function will no longer be saturated, so the derivative $\\sigma^\\prime(z)$ will be much larger, resulting in a larger value of the 2nd term in the update equation.\n",
    "\n",
    "**(2)** Assuming the weight decay term dominates in the first few epochs, we have for the update equation:\n",
    "  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w &\\rightarrow \\left (1 - \\frac{\\eta \\lambda}{n}\\right)w\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "i.e\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta w = \\frac{-\\eta \\lambda}{n}w \\\\\n",
    "\\frac{\\delta w}{w} = \\frac{-\\eta \\lambda}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the limit that $\\delta w\\rightarrow 0$, this yields\n",
    "  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dw}{w} = \\frac{-\\eta \\lambda}{n}dm \\\\\n",
    "\\int^w \\frac{dw^\\prime}{w} = \\int^{m} \\frac{-\\eta \\lambda}\n",
    "{n}dm^\\prime \\\\\n",
    "\\ln \\left(w(m)\\right ) = \\frac{-\\eta \\lambda}{n}m + c \\\\\n",
    "w(m) = A_0 \\exp \\left (\\frac{-\\eta \\lambda}{n}m \\right )\n",
    "\\end{align}\n",
    "$$\n",
    "    \n",
    "where the term $m$, denotes the integer $m^{th}$ epoch of training. Since $w(m=0)=w_0$, we know that $w$ must therefore be,\n",
    "  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w(m) = w_0 \\exp\\left (\\frac{-\\eta \\lambda}{n} m \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "    \n",
    "i.e provided that $\\eta \\lambda \\lt \\lt n$, the weights decay exponentially with a decay factor of $-\\eta \\lambda/n$ per epoch.\n",
    "  \n",
    "**(3)**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1250ff56634842d3953c3ae69bbb5a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c664db8d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "w = []\n",
    "n = 100\n",
    "w0 = np.random.randn(n,1)\n",
    "w.append(np.sort(w0,axis=0))\n",
    "for i in range(1,300):\n",
    "    w.append(w[i-1]*np.exp(-1.*1./n))\n",
    "ws = [stats.norm.pdf(i, np.mean(i), np.std(i)) for i in w]\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(w[0],ws[0],'-o',label='Epoch 0')\n",
    "for i in range(1,300,30):\n",
    "    ax1.plot(w[i],ws[i],'-o',label='Epoch {}'.format(i))\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c6672998b0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot([np.std(i) for i in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Handwriting recognition revisited: the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     25,
     41,
     67,
     300,
     315,
     325,
     329
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"network2.py\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "An improved version of network.py, implementing the stochastic\n",
    "gradient descent learning algorithm for a feedforward neural network.\n",
    "Improvements include the addition of the cross-entropy cost function,\n",
    "regularization, and better initialization of network weights.  Note\n",
    "that I have focused on making the code simple, easily readable, and\n",
    "easily modifiable.  It is not optimized, and omits many desirable\n",
    "features.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\" Epoch %s training complete\" % j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta*lmbda/n)*np.sign(w)-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "*Modify the code above to implement L1 regularization, and use L1 regularization to classify MNIST digits using a 30 hidden neuron network. Can you find a regularization parameter that enables you to do better than running unregularized?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     2,
     15,
     19,
     21,
     26,
     39,
     43,
     45
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NetworkL1(Network):\n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "      Network.__init__(self, sizes, cost=CrossEntropyCost)\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "class NetworkL0(Network):\n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "      Network.__init__(self, sizes, cost=CrossEntropyCost)\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'DeepLearningPython'\n",
      "E:\\Google Drive\\Programming\\Neural networks and deep learning\\DeepLearningPython\n"
     ]
    }
   ],
   "source": [
    "cd \"DeepLearningPython\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader as mnist\n",
    "training_data, validation_data, test_data = mnist.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     3,
     15
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 915.5448535152995\n",
      "Accuracy on training data: 47071 / 50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d999ae6e0d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#L1 Regularized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetworkL1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCrossEntropyCost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;31m# Unregularized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mnet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetworkL0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCrossEntropyCost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-68ad37692543>\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, lmbda, evaluation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy, early_stopping_n)\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy on training data: {} / {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmonitor_evaluation_cost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[0mevaluation_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cost on evaluation data: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-68ad37692543>\u001b[0m in \u001b[0;36mtotal_cost\u001b[1;34m(self, data, lmbda, convert)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorized_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-68ad37692543>\u001b[0m in \u001b[0;36mfeedforward\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;34m\"\"\"Return the output of the network if ``a`` is input.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "#L1 Regularized\n",
    "net = NetworkL1([784,30,10],cost=CrossEntropyCost)\n",
    "results = net.SGD(training_data, 20, 10, 0.5, 1, validation_data, True, True, True,True)\n",
    "# Unregularized\n",
    "net2 = NetworkL0([784,30,10],cost=CrossEntropyCost)\n",
    "results = net2.SGD(training_data, 20, 10, 0.5, 0, validation_data, True, True, True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "*Take a look at the `Network.cost_derivative()` method in network.py. That method was written for the quadratic cost. How would you rewrite the method for the cross-entropy cost? Can you think of a problem that might arise in the cross-entropy version? In network2.py we've eliminated the `Network.cost_derivative()` method entirely, instead incorporating its functionality into the `CrossEntropyCost.delta()` method. How does this solve the problem you've just identified?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**A)** In the previous version of *network.py*, the definition of the cost function was declared within the `network` class; as this function must be defined in a fixed way, there is no option to choose which cost function it uses. By declaring separate classes for the cost functions, as in *network2.py* and specifying this as an argument to the `__init__` method for the the network class, we can choose between cost functions and add new ones, without breaking existing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###### Q)\n",
    "\n",
    "*Modify network2.py so that it implements early stopping using a no-improvement-in-$n$ epochs strategy, where $n$ is a parameter that can be set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-358fe7db9df6>, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-358fe7db9df6>\"\u001b[1;36m, line \u001b[1;32m59\u001b[0m\n\u001b[1;33m    if k>epochs/3 and evaluation_accuracy[-1] < evaluation_accuracy[-2]\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        count = 0\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost)) \n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n)) \n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost)) \n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data)) \n",
    "                if k>epochs/3 and evaluation_accuracy[-1] < evaluation_accuracy[-2]\n",
    "                if evaluation_accuracy[-1] < evaluation_accuracy[-2]:\n",
    "                    count += 1\n",
    "                if k > epochs / 3. and count > 10:\n",
    "                    k = epochs\n",
    "                    print(\"Stopped early due to no improvement over\") \n",
    "            print()\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "200.26px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
