{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Chapter 1: Using neural nets to recognize handwritten digits </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "**Q) Sigmoid neurons simulating perceptrons, part I:** *Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c>0. Show that the behaviour of the network doesn't change.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)**\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sum_{j} c \\cdot w_{j}x_{j} + c \\cdot b & = c  \\left( \\sum_{j} w_{j}x_{j} + b \\right) \\\\\n",
    "& = \\left|c\\right|  \\left( \\sum_{j} w_{j}x_{j} + b \\right) \\\\\n",
    "& = \\left|c\\right| \\cdot p(z)\n",
    "\\end{eqnarray} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q) Sigmoid neurons simulating perceptrons, part II**\n",
    "*Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w\\cdot x + b \\ne 0$ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c > 0$ Show that in the limit as $c \\rightarrow \\infty$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w\\cdot x + b = 0$ for one of the perceptrons?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)**\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{1+e^{(-\\sum_{j} c \\cdot w_{j}x_{j} - c \\cdot b)}} & = \\frac{1}{1+e^{- c (\\sum_{j} w_{j}x_{j} + b)}} \\\\\n",
    "& = \\frac{1}{1+Ae^{(\\sum_{j} w_{j}x_{j} + b)}}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $A = e^{-c}$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lim_{c \\rightarrow \\infty} A = e^{-c} =\n",
    "\\begin{cases}\n",
    "\\text{$\\infty$ when $c < 0$}\\\\\n",
    "\\text{$0$ when $c > 0$}\n",
    "\\end{cases} \\\\\n",
    "\\equiv p(z)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "i.e the same as the perceptrons. However, when $w \\cdot x + b = 0$,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{1+e^{(-\\sum_{j} c \\cdot w_{j}x_{j} - c \\cdot b)}} & = \\frac{1}{1+e^{- c (\\sum_{j} w_{j}x_{j} + b)}} \\\\\n",
    "& = \\frac{1}{1+e^{0}} \\\\\n",
    "& = \\frac{1}{2} \\not\\equiv p(z)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "**Q) Learning with gradient descent, part I:** *Prove the assertion of the last paragraph.*\n",
    "\n",
    ">\"Indeed, there's even a sense in which gradient descent is the optimal strategy for searching for a minimum. Let's suppose that we're trying to make a move Δv in position so as to decrease C as much as possible. This is equivalent to minimizing ΔC≈∇C⋅Δv. We'll constrain the size of the move so that ∥Δv∥=ϵ for some small fixed ϵ>0. In other words, we want a move that is a small step of a fixed size, and we're trying to find the movement direction which decreases C as much as possible. It can be proved that the choice of Δv which minimizes ∇C⋅Δv is Δv=−η∇C, where η=ϵ/∥∇C∥ is determined by the size constraint ∥Δv∥=ϵ. So gradient descent can be viewed as a way of taking small steps in the direction which does the most to immediately decrease C.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Hint: If you're not already familiar with the Cauchy-Schwartz inequality, you may find it helpful to familiarize yourself with it.*\n",
    "\n",
    "**A)**\n",
    "\n",
    "The Cauchy-Schwartz inequality states that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{| \\langle \\textbf{u, v} \\rangle |}^{2} \\leq \\langle \\textbf{u,u} \\rangle \\cdot \\langle \\textbf{v, v} \\rangle \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "i.e the length of the inner product of two vectors will always be less than or equal to the inner product of the lengths of the indivual vectors. An alternative way of writing this is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "| \\langle \\textbf{u, v} \\rangle | \\leq ||\\textbf{u}|| ||\\textbf{v}||\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which reflects the fact that the magnitude of the dot product of two vectors will always be less than or equal to the product of the individual vector magnitudes, i.e the ratio of the two must always be at most one, in the case that $u$ and $v$ are in the same direction, ($\\hat{\\textbf{u}}=\\hat{\\textbf{v}}$) or less than one:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{| \\langle \\textbf{u, v} \\rangle |}{||\\textbf{u}|| ||\\textbf{v}||} \\leq 1 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "i.e\n",
    "$$\n",
    "\\begin{equation}\n",
    "{| \\langle \\hat{\\textbf{u}}, \\hat{\\textbf{v}} \\rangle |} \\leq 1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "if $\\Delta C$ is approximately equal to\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla C \\cdot \\Delta v \\leq ||\\nabla C|| ||\\Delta v|| \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "this implies that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "||\\hat{\\nabla C} \\cdot \\hat{\\Delta v}|| \\leq 1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The value of $\\Delta C$ that reduces the cost function the most must therefore be when $\\hat{\\nabla C} \\cdot \\hat {\\Delta v}$ = -1, i.e $\\hat{\\Delta v} = - \\hat{\\nabla C}$. In order for the equality to be satisfied, we must have\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\Delta v = - \\frac{\\nabla C}{|\\nabla C|}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the real world, our value of $\\nabla C$ will be only an approximate value, as we calculated it to first order only, so we will only want to take a small step in this direction, hence the addition of the prefactor $\\epsilon$.\n",
    "\n",
    "**Q) Learning with gradient descent, part II:** *I explained gradient descent when $C$ is a function of two variables, and when it's a function of more than two variables. What happens when $C$ is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?*\n",
    "\n",
    "**A)** When C is a function of just one variable, the geometric interpretation is simply a smooth curve on a graph f(x), where we try to find the minimum value of f(x). The physical equivalent is a bead on a curvy wire, which is suspended such that the height at point x on the string is f(x). Gradient descent is the equivalent of pushing the bead in the direction that will cause it to fall down the wire, as opposed to pushing it up.\n",
    "\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> An implementation of a network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Google Drive\\Programming\\Neural networks and deep learning\\DeepLearningPython\n"
     ]
    }
   ],
   "source": [
    "cd DeepLearningPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this code, the list sizes contains the number of neurons in the respective layers. So, for example, if we want to create a Network object with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer, we'd do this with the code:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "sizes = [2, 3, 1]\n",
      "weights = [array([[ 2.04560037, -0.16860254],\n",
      "       [ 0.62497871, -0.09198063],\n",
      "       [-0.88374234,  0.1298795 ]]), array([[ 1.89807656,  1.66205714, -0.69854201]])]\n",
      "biases = [array([[ 0.90920733],\n",
      "       [ 0.08960849],\n",
      "       [-0.21954186]]), array([[0.29835514]])]\n"
     ]
    }
   ],
   "source": [
    "from network import *\n",
    "\n",
    "net = Network([2,3,1])\n",
    "print('num_layers = ' + str(net.num_layers))\n",
    "print('sizes = ' + str(net.sizes))\n",
    "print('weights = ' + str(net.weights))\n",
    "print('biases = ' + str(net.biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.89807656,  1.66205714, -0.69854201]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The biases and weights in the Network object are all initialized randomly, using the Numpy np.random.randn function to generate Gaussian distributions with mean 0 and standard deviation 1. This random initialization gives our stochastic gradient descent algorithm a place to start from.*\n",
    "\n",
    "*Note that the Network initialization code assumes that the first layer of neurons is an input layer, and omits to set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.*\n",
    "\n",
    "*Note also that the biases and weights are stored as lists of Numpy matrices. So, for example net.weights[1] is a Numpy matrix storing the weights connecting the second and third layers of neurons. (It's not the first and second layers, since Python's list indexing starts at 0.)*\n",
    "\n",
    "*Since net.weights[1] is rather verbose, let's just denote that matrix $w$. It's a matrix such that $w_{jk}$ is the weight for the connection between the $k^{th}$ neuron in the second layer, and the $j^{th}$ neuron in the third layer. This ordering of the $j$ and $k$ indices may seem strange - surely it'd make more sense to swap the and indices around? The big advantage of using this ordering is that it means that the vector of activations of the third layer of neurons is:*\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^{\\prime} = \\sigma(wa+b)\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "*There's quite a bit going on in this equation, so let's unpack it piece by piece. $a$ is the vector of activations of the second layer of neurons. To obtain $a^{\\prime}$ we multiply $a$ by the weight matrix $w$, and add the vector $b$ of biases. We then apply the function $\\sigma$ elementwise to every entry in the vector $wa+b$. (This is called vectorizing the function $\\sigma$.)* \n",
    "\n",
    "*This works in a similar way to a recursive algorithm, as we take the output from the $n^{th}$ layer, $a^{\\prime}$, and redefine it as $a$, which becomes the input to the ${(n+1)}^{th}$ layer and so on.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "**Q)** *Write out Equation (11) in component form, and verify that it gives the same result as the rule (2) for computing the output of a sigmoid neuron.*\n",
    "\n",
    "**A)**\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "a^{\\prime} &= \\frac{1}{1+exp(-(wa + b))} \\\\\n",
    "&= \\frac{1}{1+exp(-(w_{1,0}\\cdot x_{1,0} + w_{1,1} \\cdot x_{1,1} + w_{1,2} \\cdot x_{1,2} + b_{1,0} + b_{1,1} + b_{1,2})}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementing our network to classify digits</h3>\n",
    "\n",
    "Defining some classes and functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Breaking down the *feedforward()* algorithm: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import *\n",
    "net = Network(sizes=(2,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83411024]]\n",
      "[[0.83411024]]\n"
     ]
    }
   ],
   "source": [
    "# Our example neural net is shape (2,3,1), so let's set the input vector a = (1,0),\n",
    "# which has the correct dimensions (2,1)\n",
    "a = np.ones([2,1],dtype=int)\n",
    "a[1] = 0\n",
    "\n",
    "# Calculate the output of the first layer of neurons (after the input layer of course)\n",
    "z = np.dot(net.weights[0], a) + net.biases[0]\n",
    "# Now using equation 11, a -> a'\n",
    "a = sigmoid(z)\n",
    "# a is recast as the output of the first layer, and now becomes the input of the second layer\n",
    "z = np.dot(net.weights[1], a) + net.biases[1]\n",
    "# Our final layer is only one neuron, so this is the final answer\n",
    "a = sigmoid(z)\n",
    "\n",
    "print(a)\n",
    "\n",
    "a = np.ones([2,1],dtype=int)\n",
    "a[1] = 0\n",
    "print(net.feedforward(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breaking down the `backprop()` algorithm:\n",
    "\n",
    "We will try to illustrate the back propagation algorithm here by trying to train a mini network to become a 2 bit adder!\n",
    "\n",
    "<img src='capture_Sun_Nov__4 15_20_07_GMT_2018.png'>\n",
    "\n",
    "We expect all neurons in the network to be NAND gates (except the input layer), therefore with weights of -0.66 and biases of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biases:\n",
      "[array([[-0.78009436],\n",
      "       [-1.15196502],\n",
      "       [ 1.06436551]]), array([[-0.04585299],\n",
      "       [ 1.1120488 ],\n",
      "       [-2.43608196]]), array([[-0.00889339],\n",
      "       [ 1.89750115]])]\n",
      "Weights:\n",
      "[array([[ 0.94952528,  0.58164616],\n",
      "       [-0.02198432, -0.6131658 ],\n",
      "       [ 0.36183138,  0.85065596]]), array([[-1.82711695,  1.38677051,  0.5935674 ],\n",
      "       [ 1.51856497,  0.0621353 ,  1.88881572],\n",
      "       [-0.58063738, -0.21220037,  0.70110643]]), array([[-0.25502273, -0.22535264, -0.14794123],\n",
      "       [-0.88444068,  0.37041323,  0.98856683]])]\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes=(2,3,3,2))\n",
    "x = np.ones([2,1],dtype=int)\n",
    "# Check the random initial weights and biases, leading to incorrect output\n",
    "print('Biases:')\n",
    "print(str(net.biases)) \n",
    "print('Weights:') \n",
    "print(str(net.weights)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twobitadder(x):\n",
    "    y = np.zeros([2,1],dtype=int)\n",
    "    if x[0] & x[1]:\n",
    "        y[1] = 1\n",
    "        return y\n",
    "    if x[0] + x[1] == 1:\n",
    "        y[0] = 1\n",
    "        return y\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "def train(net, n_samples, training_rate):\n",
    "    myset = [0,1]\n",
    "    n = 0\n",
    "    while n < n_samples:\n",
    "        x = np.zeros([2,1], dtype=int)\n",
    "        x[0], x[1] = np.random.choice(myset), np.random.choice(myset)\n",
    "        y = twobitadder(x)\n",
    "        nabla_b, nabla_w = mybackprop(net, x, y)\n",
    "        myupdate(net,x,y,training_rate, nabla_b, nabla_w)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backprop() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mybackprop(net,x,y):\n",
    "    ###### Set up arrays for the gradient values of the weights and biases\n",
    "    ######################################################################\n",
    "    nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
    "    \n",
    "    ###### Feed Forward algorithm\n",
    "    ######################################################################\n",
    "    # activation will represent the output of each layer of neurons. At the start,\n",
    "    # we assign it the value of the input (image), as this is the output of the first (input) layer\n",
    "    activation = x\n",
    "    # This list contains the output of each layer of neurons.\n",
    "    activations =[x]\n",
    "    zs = [] # w*x + b for each layer\n",
    "    for b, w in zip(net.biases, net.weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    ###### Backpropagation algorithm\n",
    "    ######################################################################\n",
    "    # Backpropagation equation 1 (BP1): d^L = dC/da^L * sigma_prime(z^L)\n",
    "    delta = net.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    # Backpropagation equation 3 (BP3) dC/db^l_j = d^l_j\n",
    "    nabla_b[-1] = delta\n",
    "    # Backpropagation equation 4 (BP4) dC/dw^l_j = a^(l-1)_k * d^l_j\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "    for l in range(2, net.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        # Backpropagation equation 2 (BP2) d^l = w^(l+1) * d^(l+1) * sigma_prime(z^l)\n",
    "        delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
    "        # (BP3)\n",
    "        nabla_b[-l] = delta\n",
    "        # (BP4)\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    \n",
    "    return nabla_b, nabla_w\n",
    "\n",
    "def myupdate(net,x,y,eta, nabla_b, nabla_w):\n",
    "    delta_nabla_b = nabla_b\n",
    "    delta_nabla_w = nabla_w\n",
    "    nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "    nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    # Update weights: w -> w - eta*nabla_w\n",
    "    net.weights = [w-eta*nw\n",
    "                            for w, nw in zip(net.weights, nabla_w)]\n",
    "    # Update biases: b -> b - eta*nabla_b\n",
    "    net.biases = [b-eta*nb\n",
    "                   for b, nb in zip(net.biases, nabla_b)]\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]), array([[-0.00885138, -0.00676261, -0.0209393 ],\n",
      "       [ 0.00020897,  0.00015966,  0.00049436],\n",
      "       [ 0.00247437,  0.00189046,  0.00585349]]), array([[0.05289275, 0.09360585, 0.0102642 ],\n",
      "       [0.0537421 , 0.09510896, 0.01042902]])]\n"
     ]
    }
   ],
   "source": [
    "myset = [0,1]\n",
    "x = np.zeros([2,1], dtype=int)\n",
    "x[0], x[1] = np.random.choice(myset), np.random.choice(myset)\n",
    "y = twobitadder(x)\n",
    "nabla_b, nabla_w = mybackprop(net, x, y)\n",
    "myupdate(net,x,y,0.3, nabla_b, nabla_w)\n",
    "print(nabla_w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6ee1bb0f3fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-c7d9c06b6cd1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, n_samples, training_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwobitadder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmybackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmyupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-6ca46441d778>\u001b[0m in \u001b[0;36mmybackprop\u001b[1;34m(net, x, y)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mnabla_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# (BP4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mnabla_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net,500000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biases:\n",
      "[array([[-6.24486324],\n",
      "       [ 1.85870819],\n",
      "       [ 1.24236802]]), array([[ 2.57020417],\n",
      "       [-0.79578576],\n",
      "       [-0.14171694]]), array([[-5.7148517 ],\n",
      "       [ 2.36117744]])]\n",
      "Weights:\n",
      "[array([[ 4.17927722,  4.16456449],\n",
      "       [-4.57224152, -4.57183361],\n",
      "       [ 1.10289714,  1.17845907]]), array([[-9.21685171,  5.04791816,  2.04029513],\n",
      "       [ 2.55232309,  2.797352  , -0.15165224],\n",
      "       [ 1.12767362, -6.21552637,  3.49963428]]), array([[ 6.52877518, -6.61371651,  6.63374774],\n",
      "       [-8.91311716,  1.33238442,  1.12865682]])]\n",
      "\n",
      "\n",
      "Output:\n",
      "Sum bit: [0.01439783]\n",
      "Carry bit: [0.00452371]\n",
      "\n",
      "\n",
      "Correct result:\n",
      "Sum bit: [0]\n",
      "Carry bit: [0]\n"
     ]
    }
   ],
   "source": [
    "print('Biases:') \n",
    "print(str(net.biases)) \n",
    "print('Weights:') \n",
    "print(str(net.weights)) \n",
    "print('\\n') \n",
    "print('Output:') \n",
    "x1, x2 = net.feedforward(x)\n",
    "print('Sum bit: ' + str(x1)) \n",
    "print('Carry bit: ' + str(x2)) \n",
    "print('\\n') \n",
    "print('Correct result:') \n",
    "y1, y2 = twobitadder(x)\n",
    "print('Sum bit: ' + str(y1)) \n",
    "print('Carry bit: ' + str(y2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = 1\n",
    "x[1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
